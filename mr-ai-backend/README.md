Got it âœ…
Hereâ€™s a fully polished, best-practices **README.md** with consistent English, emojis, and improved formatting for clarity and developer friendliness.

---

# ğŸ¤– MR-AI Backend

A **self-hosted AI-powered backend** for **automated Merge Request (MR) reviews** using custom AI models.
Supports integration with **GitHub**, **GitLab**, and other Git providers via **SSH**.

---

## ğŸ“‚ Project Structure

```bash
â”œâ”€â”€ api/                # API server logic
â”œâ”€â”€ code_data/          # Cloned repository data and processing artifacts
â”œâ”€â”€ vector-lib/         # Use to work with vector data
â”œâ”€â”€ graph-prepare/      # Syntax tree and graph generation
â”œâ”€â”€ services/           # Service logic and helpers
â”œâ”€â”€ ssh_keys/           # SSH keys for Git access
â”œâ”€â”€ .env                # Environment configuration
```

---

## âš™ï¸ Environment Configuration

Configuration is done via a `.env` file or environment variables:

```env
######## General ########
PROJECT_NAME=test_project      # Unique folder name per project
API_ADDRESS=0.0.0.0:3000       # API server binding address

######## Qdrant ########
QDRANT_HTTP_PORT=6333          # Qdrant HTTP API port
QDRANT_GRPC_PORT=6334          # Qdrant gRPC API port
QDRANT_URL=http://localhost:6333
QDRANT_COLLECTION=mr_ai_code
QDRANT_DISTANCE=Cosine
QDRANT_BATCH_SIZE=256

######## Graph / Export ########
GRAPH_EXPORT_DIR_NAME=graphs_data
GRAPH_EXCLUDE_GENERATED=true
GRAPH_GENERATED_GLOBS=**/*.g.dart,**/*.freezed.dart

######## Embeddings ########
OLLAMA_URL=http://localhost:7869
EMBEDDING_MODEL=dengcao/Qwen3-Embedding-0.6B:Q8_0
EMBEDDING_DIM=1024 # Verify via curl API (see below)

######## Chunking ########
CHUNK_MAX_CHARS=4000
CHUNK_MIN_CHARS=16

######## Concurrency ########
EMBEDDING_CONCURRENCY=4
```

> ğŸ’¡ You can run **multiple projects** by changing `PROJECT_NAME`.

---

## ğŸ” SSH Setup for Git Access

The service uses **SSH keys** for secure access to private repositories.

### 1ï¸âƒ£ Generate SSH Key

```bash
ssh-keygen -t ed25519 -C "bot@mr-ai.com" -f ./ssh_keys/bot_key
```

This will create:

* **Private key:** `ssh_keys/bot_key`
* **Public key:** `ssh_keys/bot_key.pub`

âš ï¸ **Never commit** your private key to version control.

---

### 2ï¸âƒ£ Add Public Key to Your Git Provider

**GitHub**

1. Go to **Settings â†’ SSH and GPG Keys â†’ New SSH Key**
2. Paste the contents of `ssh_keys/bot_key.pub`

**GitLab**

1. Go to **User Settings â†’ SSH Keys**
2. Paste the contents of `ssh_keys/bot_key.pub`

---

### 3ï¸âƒ£ Accept SSH Host Fingerprint (Required for `libgit2`)

```bash
ssh-keyscan gitlab.com >> ~/.ssh/known_hosts
```

---

## ğŸš€ Running the Service

1. Set up `.env`
2. Configure SSH access
3. Start the service:

```bash
cargo run --release
```

---

## ğŸ“ .gitignore Best Practices

```gitignore
# Dynamic repo data
code_data/*
!code_data/.gitkeep

# Private SSH keys
ssh_keys/*
!ssh_keys/.gitkeep
```

---

## ğŸŒ³ Syntax Tree & Graph Generation

Uses [Tree-sitter](https://tree-sitter.github.io/tree-sitter) to parse code into syntax trees and build graphs.

**Languages:**

* âœ… Dart *(ready)*
* ğŸš§ Rust *(in progress)*
* ğŸš§ Python *(in progress)*
* ğŸš§ JavaScript *(in progress)*
* ğŸš§ TypeScript *(in progress)*

---

## ğŸ“¦ Saved Artifacts

Stored at:

```
code_data/<project_name>/graphs_data/<timestamp>/
```

Includes:

* `graph.graphml` â†’ Import into [Gephi](https://gephi.org/)
* `ast_nodes.jsonl` â†’ Abstract syntax tree nodes
* `graph_nodes.jsonl` â†’ Graph node data
* `graph_edges.jsonl` â†’ Graph edge data
* `summary.json` â†’ Summary metadata

---

## ğŸ›  API Endpoints

1. **Upload Project Data**
   `POST /upload_project_data` â€” Send repository data.
2. **Learn Code & Generate Graphs**
   `POST /learn_code` â€” Build graph representation of code.

---

## ğŸ³ Quick Start with Docker Compose

```bash
docker compose up -d
# Open UI / test API:
open http://localhost:6333
# Health check:
curl -s localhost:6333/readyz
```

---

## âš¡ GPU Builds (Linux Only)

GPU builds available for:

* **NVIDIA** â€” `qdrant/qdrant:gpu-nvidia-latest`
* **AMD ROCm** â€” `qdrant/qdrant:gpu-amd-latest`

> For full Docker Compose GPU configuration, see the Qdrant docs:
> [Qdrant GPU Guide](https://qdrant.tech/documentation/gpu/)

---

## ğŸ§ª Testing Embedding Model

```bash
docker exec -it ollama ollama pull dengcao/Qwen3-Embedding-0.6B:Q8_0
docker exec -it ollama ollama pull qwen3:32b
```

Check model dimension (`EMBEDDING_DIM`):

```bash
curl --location 'http://localhost:11434/api/embed' \
  --header 'Content-Type: application/json' \
  --data '{
    "model": "dengcao/Qwen3-Embedding-0.6B:Q8_0",
    "input": "hello"
  }'
```

---

## ğŸ¤ Contributing

Contributions for additional language support, performance improvements, and bug fixes are welcome!
Please open an issue or PR.

---

## ğŸ“œ License

MIT â€” Free to use and modify.

---

Do you want me to also **add a badges section** (build status, Docker pulls, version, etc.) so it looks even more professional? That would make this README really pop.
