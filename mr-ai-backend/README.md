# ğŸ¤– MR-AI Backend

A **self-hosted backend** for automated Merge Request (MR) reviews powered by **local/custom AI models**.
It integrates with **GitHub**, **GitLab**, and other Git providers over **SSH**.
By default it uses [Ollama](https://ollama.com) for local inference and [Qdrant](https://qdrant.tech) as a vector database.

---

## ğŸ§­ Table of Contents

* [Capabilities](#-capabilities)
* [Architecture](#-architecture)
* [Requirements](#-requirements)
* [Quick Start (Docker)](#-quick-start-docker)
* [Local Install (Rust)](#-local-install-rust)
* [Environment Setup](#-environment-setup)
* [SSH Access to Git](#-ssh-access-to-git)
* [Workflow](#-workflow)
* [API (cURL Examples)](#-api-curl-examples)
* [AST/Graph Generation](#-astgraph-generation)
* [AI LLM Service Profiles](#-ai-llm-service-profiles)
* [Practices & Recommendations](#-practices--recommendations)
* [.gitignore](#-gitignore)
* [Model Management](#-model-management)
* [Qdrant GPU Images](#-qdrant-gpu-images)
* [Contributing & License](#-contributing--license)

---

## âœ¨ Capabilities

* ğŸ” Clones repositories over SSH and prepares **RAG** context
* ğŸŒ³ Builds **ASTs** and **code graphs** with Tree-sitter
* ğŸ§  Indexes source code into **Qdrant** (vector DB)
* ğŸ’¬ Answers questions with **LLM profiles** (fast / slow / embedding)
* ğŸ”— Supports GitHub/GitLab and others via SSH
* ğŸ©º Includes a built-in **health service** for LLM endpoints

---

## ğŸ—‚ï¸ Architecture

> Key libs and directories currently in the repo:

```bash
â”œâ”€â”€ ai-llm-service/      # Shared LLM services: providers (Ollama/OpenAI), health, profile manager
â”œâ”€â”€ api/                 # HTTP API server (endpoints calling contextor + services)
â”œâ”€â”€ code_data/           # Project data: repo clones, embeddings, graph artifacts
â”œâ”€â”€ codegraph-prep/      # Build ASTs and code graphs (Tree-sitter based)
â”œâ”€â”€ contextor/           # Orchestration: query â†’ RAG â†’ LLM (fast/slow/embedding)
â”œâ”€â”€ mr-reviewer/         # MR review logic (rules execution, scoring, escalation)
â”œâ”€â”€ rag-store/           # Transform AST/graph into vector DB payloads (Qdrant)
â”œâ”€â”€ rules/               # LLM prompt rules: global & per-language policies
â”œâ”€â”€ services/            # Service wiring (e.g., repo fetchers, providers glue)
â”œâ”€â”€ src/                 # Main binaries / entrypoints
â”œâ”€â”€ ssh_keys/            # SSH keys for repo access (do not commit private keys)
â”œâ”€â”€ .env                 # Project environment variables
â”œâ”€â”€ bootstrap_ollama.sh  # Helper to spin up dependencies (Ollama + Qdrant)
â”œâ”€â”€ docker-compose.yml   # Local stack: Ollama + Qdrant
```

---

## ğŸ§© Requirements

* ğŸ³ Docker + Docker Compose
* ğŸ¦€ Rust (stable) for local runs
* ğŸ“¦ 8â€“30 GB free disk space (models + embeddings + indices)
* ğŸ” SSH access to your Git repositories

---

## ğŸš€ Quick Start (Docker)

1. **Prepare `.env`** (see below).
2. **Start dependencies** (Ollama + Qdrant):

   ```bash
   chmod +x ./bootstrap_ollama.sh
   ./bootstrap_ollama.sh
   ```
3. **Check Qdrant UI**: [http://localhost:6333](http://localhost:6333)
4. **Run the API** (locally):

   ```bash
   cargo run --release
   ```

---

## ğŸ›  Local Install (Rust)

```bash
rustup default stable
cargo run --release
```

---

## âš™ï¸ Environment Setup

> Minimal, clean, and consistent with the current codebase.
> Note: `QDRANT_URL` intentionally remains `http://localhost:6334` per your setup.

```env
############################
# ğŸ”¹ Project
############################
PROJECT_NAME=project_x
API_ADDRESS=0.0.0.0:3000

############################
# ğŸ”¹ Ollama / LLM
############################
# Local or remote Ollama endpoint
OLLAMA_URL=http://localhost:11434
# Models (fast for drafting, slow for refine/verify)
OLLAMA_MODEL_FAST_MODEL=qwen3:14b
OLLAMA_MODEL=qwen3:32b

############################
# ğŸ”¹ Embeddings
############################
EMBEDDING_MODEL=dengcao/Qwen3-Embedding-0.6B:Q8_0
EMBEDDING_DIM=1024
EMBEDDING_CONCURRENCY=4

############################
# ğŸ”¹ Qdrant (Vector DB)
############################
QDRANT_HTTP_PORT=6333
QDRANT_GRPC_PORT=6334
QDRANT_URL=http://localhost:6334
QDRANT_COLLECTION=mr_ai_code
QDRANT_DISTANCE=Cosine
QDRANT_BATCH_SIZE=256

############################
# ğŸ”¹ Chunking
############################
CHUNK_MAX_CHARS=4000
CHUNK_MIN_CHARS=16

############################
# ğŸ”¹ Debug
############################
# RUST_BACKTRACE=1
RUST_LOG=mr_reviewer=debug,contextor=debug,rag_store=debug,reqwest=info
```

Optional (GitLab integration):

```env
# GITLAB_API_BASE=https://gitlab.com/api/v4
# GITLAB_TOKEN=__REDACTED__
# TRIGGER_SECRET=super-secret
```

---

## ğŸ” SSH Access to Git

```bash
ssh-keygen -t ed25519 -C "bot@mr-ai.com" -f ./ssh_keys/bot_key
ssh-keyscan gitlab.com >> ~/.ssh/known_hosts
# Repeat for github.com / your host as needed
```

Add `ssh_keys/bot_key.pub` in your Git providerâ€™s SSH keys page.
**Never commit** `ssh_keys/bot_key`.

---

## ğŸ§ª Workflow

1. Start **Ollama + Qdrant** â†’ `./bootstrap_ollama.sh`
2. Run **API** â†’ `cargo run --release`
3. Attach repository â†’ `POST /upload_project_data`
4. Learn code (index) â†’ `POST /learn_code`
5. Prepare graph â†’ `POST /prepare_graph`
6. Initialize Qdrant â†’ `POST /prepare_qdrant`
7. Ask questions â†’ `POST /ask_question`

---

## ğŸ›°ï¸ API (cURL Examples)

**Ask a code question**

```bash
curl --silent 'http://0.0.0.0:3000/ask_question' \
  -H 'Content-Type: application/json' \
  -d '{"question": "Where is the main navigation bar defined?"}'
```

**Attach repository**

```bash
curl --silent 'http://0.0.0.0:3000/upload_project_data' \
  -H 'Content-Type: application/json' \
  -d '{"urls": ["git@gitlab.com:user/project.git"]}'
```

**Learn code**

```bash
curl --silent 'http://0.0.0.0:3000/learn_code'
```

**Prepare graph**

```bash
curl --silent 'http://0.0.0.0:3000/prepare_graph'
```

**Initialize Qdrant**

```bash
curl --silent 'http://0.0.0.0:3000/prepare_qdrant'
```

---

## ğŸŒ³ AST/Graph Generation

We rely on **Tree-sitter** to parse code and build graph structures.

**Artifacts** live in:

```
code_data/<PROJECT_NAME>/graphs_data/<timestamp>/
```

Contents:

* `graph.graphml` â€” open with [Gephi](https://gephi.org/)
* `ast_nodes.jsonl`, `graph_nodes.jsonl`, `graph_edges.jsonl`
* `summary.json` â€” metadata

---

## ğŸ§  AI LLM Service

The `ai-llm-service` crate provides a **shared manager** for three LLM profiles:

* **fast** â€” quick drafting
* **slow** â€” refinement/verification (falls back to fast if not set)
* **embedding** â€” vector embeddings

### Features

* **Provider-agnostic** â€” works with **Ollama** and **OpenAI**
* **Client caching** â€” reuses HTTP clients per `(provider, endpoint, model, key, timeout)`
* **Async + Arc-safe** â€” designed for reuse across tasks and modules
* **Built-in health checks** â€” probes Ollama (`/api/tags`) and OpenAI (`/v1/models`)
* **Unified errors** â€” normalized error model with snippets
* **Structured logging** â€” uses [`tracing`](https://docs.rs/tracing), tagged `[AI LLM Service]`

---

### Example

```rust
use std::sync::Arc;
use ai_llm_service::{
    service_profiles::LlmServiceProfiles,
    llm::LlmModelConfig,
    config::llm_provider::LlmProvider,
};

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    let fast = LlmModelConfig {
        provider: LlmProvider::Ollama,
        model: "qwen3:14b".into(),
        endpoint: "http://localhost:11434".into(),
        api_key: None,
        max_tokens: Some(512),
        temperature: Some(0.7),
        top_p: Some(0.9),
        timeout_secs: Some(30),
    };

    let slow = LlmModelConfig { model: "qwen3:32b".into(), ..fast.clone() };
    let embedding = LlmModelConfig { ..fast.clone() };

    // Create service with health checker (timeout = 10s)
    let svc = Arc::new(LlmServiceProfiles::new(fast, Some(slow), embedding, Some(10))?);

    let txt = svc.generate_fast("Hello world", None).await?;
    println!("FAST: {}", txt);

    let emb = svc.embed("Ferris").await?;
    println!("Embedding size = {}", emb.len());

    let statuses = svc.health_all().await?;
    println!("Health: {:?}", statuses);

    Ok(())
}
```

---

### API

* `generate_fast(prompt, system)` â†’ quick text generation
* `generate_slow(prompt, system)` â†’ refined generation (falls back to fast)
* `embed(input)` â†’ vector embeddings
* `health_all()` â†’ probe all distinct profiles
* `profiles()` â†’ return references to `(fast, slow, embedding)` configs

All methods return `Result<_, AiLlmError>` with normalized provider-specific errors.

---

### Logging

This library uses **structured logs** via `tracing`.
Logs include provider, endpoint, model, and error snippets, e.g.:

```
2025-09-12T12:05:33Z [DEBUG] [AI LLM Service] POST http://localhost:11434/api/generate model="qwen3:14b"
2025-09-12T12:05:34Z [ERROR] [AI LLM Service] Ollama returned 500 at /api/generate snippet="internal error"
```

**What is logged:**

* **INFO** â€” profile creation, health check results
* **DEBUG** â€” outgoing HTTP requests (`POST /api/...`)
* **ERROR** â€” non-2xx responses, decode errors, missing models
* **WARN** â€” unexpected but non-fatal conditions (e.g. missing `models` field in health check)

You can **tune log level just for this crate** in your `main` with `EnvFilter::add_directive`:

```rust
use tracing_subscriber::{EnvFilter, prelude::*};

fn init_tracing() {
    // Base filter from env or fallback
    let base = EnvFilter::try_from_default_env()
        .unwrap_or_else(|_| EnvFilter::new("info"));

    // Raise verbosity for ai-llm-service only
    let filter = base.add_directive("ai_llm_service=debug".parse().unwrap());

    tracing_subscriber::registry()
        .with(filter)
        .with(tracing_subscriber::fmt::layer())
        .init();
}
```

This way you can keep your app logs at `info` while enabling **detailed debug logs** only for `ai-llm-service`.

---

## âœ… Practices & Recommendations

* ğŸ”’ **Secrets**: never commit real tokens/keys; use secret managers or CI/CD variables
* ğŸ“¦ **Pre-pull models** into Ollama to reduce first-request latency
* ğŸ“ **Tune chunking** (size vs. accuracy trade-offs)
* ğŸ§ª **Match embedding dimension** (`EMBEDDING_DIM`) to your embedding model
* ğŸ©º Use `health_all()` during startup readiness checks

---

## ğŸ“ .gitignore

```gitignore
code_data/*
!code_data/.gitkeep
ssh_keys/*
!ssh_keys/.gitkeep
.env
target/
```

---

## ğŸ”„ Model Management

```bash
# Pull models (inside your Ollama container)
docker exec -it ollama ollama pull dengcao/Qwen3-Embedding-0.6B:Q8_0
docker exec -it ollama ollama pull qwen3:14b
docker exec -it ollama ollama pull qwen3:32b

# Validate embedding dimensionality (EMBEDDING_DIM)
curl -s http://localhost:11434/api/embed \
  -H 'Content-Type: application/json' \
  -d '{"model":"dengcao/Qwen3-Embedding-0.6B:Q8_0","input":"hello"}'
```

---

## âš¡ Qdrant GPU Images

* NVIDIA â€” `qdrant/qdrant:gpu-nvidia-latest`
* AMD ROCm â€” `qdrant/qdrant:gpu-amd-latest`

---

## ğŸ¤ Contributing & License

Contributions welcome: language support, performance tweaks, bug fixes.
License: **FSL-1.1**.
